2024-03-14 23:55:36,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-14 23:55:36,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-14 23:55:36,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-14 23:55:36,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-15 09:59:42,111:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-15 09:59:42,111:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-15 09:59:42,111:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-15 09:59:42,111:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-03-15 10:03:53,475:INFO:PyCaret ClassificationExperiment
2024-03-15 10:03:53,475:INFO:Logging name: DiseasePrediction
2024-03-15 10:03:53,475:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-03-15 10:03:53,475:INFO:version 3.3.0
2024-03-15 10:03:53,475:INFO:Initializing setup()
2024-03-15 10:03:53,475:INFO:self.USI: 3ce0
2024-03-15 10:03:53,476:INFO:self._variable_keys: {'gpu_n_jobs_param', 'USI', 'idx', 'y_train', 'target_param', 'n_jobs_param', 'y_test', 'X_test', '_available_plots', 'X', 'exp_id', 'y', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'data', 'html_param', 'gpu_param', '_ml_usecase', 'fold_groups_param', 'fold_generator', 'exp_name_log', 'logging_param', 'is_multiclass', 'fix_imbalance', 'pipeline', 'memory', 'seed'}
2024-03-15 10:03:53,476:INFO:Checking environment
2024-03-15 10:03:53,476:INFO:python_version: 3.10.9
2024-03-15 10:03:53,476:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2024-03-15 10:03:53,476:INFO:machine: AMD64
2024-03-15 10:03:53,476:INFO:platform: Windows-10-10.0.22621-SP0
2024-03-15 10:03:53,476:INFO:Memory: svmem(total=16831889408, available=8237752320, percent=51.1, used=8594137088, free=8237752320)
2024-03-15 10:03:53,476:INFO:Physical Core: 8
2024-03-15 10:03:53,476:INFO:Logical Core: 12
2024-03-15 10:03:53,476:INFO:Checking libraries
2024-03-15 10:03:53,476:INFO:System:
2024-03-15 10:03:53,476:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2024-03-15 10:03:53,476:INFO:executable: C:\Users\salvi\anaconda3\python.exe
2024-03-15 10:03:53,476:INFO:   machine: Windows-10-10.0.22621-SP0
2024-03-15 10:03:53,476:INFO:PyCaret required dependencies:
2024-03-15 10:03:53,477:INFO:                 pip: 22.3.1
2024-03-15 10:03:53,477:INFO:          setuptools: 65.6.3
2024-03-15 10:03:53,477:INFO:             pycaret: 3.3.0
2024-03-15 10:03:53,477:INFO:             IPython: 8.12.0
2024-03-15 10:03:53,477:INFO:          ipywidgets: 8.1.2
2024-03-15 10:03:53,477:INFO:                tqdm: 4.65.0
2024-03-15 10:03:53,477:INFO:               numpy: 1.24.3
2024-03-15 10:03:53,477:INFO:              pandas: 1.5.3
2024-03-15 10:03:53,477:INFO:              jinja2: 3.1.2
2024-03-15 10:03:53,477:INFO:               scipy: 1.10.1
2024-03-15 10:03:53,477:INFO:              joblib: 1.2.0
2024-03-15 10:03:53,477:INFO:             sklearn: 1.4.1.post1
2024-03-15 10:03:53,477:INFO:                pyod: 1.1.3
2024-03-15 10:03:53,477:INFO:            imblearn: 0.12.0
2024-03-15 10:03:53,477:INFO:   category_encoders: 2.6.3
2024-03-15 10:03:53,477:INFO:            lightgbm: 3.3.5
2024-03-15 10:03:53,477:INFO:               numba: 0.59.0
2024-03-15 10:03:53,477:INFO:            requests: 2.29.0
2024-03-15 10:03:53,477:INFO:          matplotlib: 3.7.1
2024-03-15 10:03:53,477:INFO:          scikitplot: 0.3.7
2024-03-15 10:03:53,477:INFO:         yellowbrick: 1.5
2024-03-15 10:03:53,477:INFO:              plotly: 5.20.0
2024-03-15 10:03:53,477:INFO:    plotly-resampler: Not installed
2024-03-15 10:03:53,477:INFO:             kaleido: 0.2.1
2024-03-15 10:03:53,477:INFO:           schemdraw: 0.15
2024-03-15 10:03:53,477:INFO:         statsmodels: 0.13.5
2024-03-15 10:03:53,477:INFO:              sktime: 0.27.0
2024-03-15 10:03:53,477:INFO:               tbats: 1.1.3
2024-03-15 10:03:53,477:INFO:            pmdarima: 2.0.4
2024-03-15 10:03:53,477:INFO:              psutil: 5.9.0
2024-03-15 10:03:53,477:INFO:          markupsafe: 2.1.1
2024-03-15 10:03:53,477:INFO:             pickle5: Not installed
2024-03-15 10:03:53,477:INFO:         cloudpickle: 2.2.1
2024-03-15 10:03:53,477:INFO:         deprecation: 2.1.0
2024-03-15 10:03:53,477:INFO:              xxhash: 3.4.1
2024-03-15 10:03:53,477:INFO:           wurlitzer: Not installed
2024-03-15 10:03:53,477:INFO:PyCaret optional dependencies:
2024-03-15 10:03:53,594:INFO:                shap: Not installed
2024-03-15 10:03:53,594:INFO:           interpret: Not installed
2024-03-15 10:03:53,594:INFO:                umap: Not installed
2024-03-15 10:03:53,594:INFO:     ydata_profiling: Not installed
2024-03-15 10:03:53,594:INFO:  explainerdashboard: Not installed
2024-03-15 10:03:53,594:INFO:             autoviz: Not installed
2024-03-15 10:03:53,594:INFO:           fairlearn: Not installed
2024-03-15 10:03:53,594:INFO:          deepchecks: Not installed
2024-03-15 10:03:53,594:INFO:             xgboost: 1.7.3
2024-03-15 10:03:53,594:INFO:            catboost: Not installed
2024-03-15 10:03:53,594:INFO:              kmodes: Not installed
2024-03-15 10:03:53,594:INFO:             mlxtend: Not installed
2024-03-15 10:03:53,594:INFO:       statsforecast: Not installed
2024-03-15 10:03:53,594:INFO:        tune_sklearn: Not installed
2024-03-15 10:03:53,594:INFO:                 ray: Not installed
2024-03-15 10:03:53,594:INFO:            hyperopt: Not installed
2024-03-15 10:03:53,594:INFO:              optuna: Not installed
2024-03-15 10:03:53,594:INFO:               skopt: 0.9.0
2024-03-15 10:03:53,594:INFO:              mlflow: Not installed
2024-03-15 10:03:53,594:INFO:              gradio: Not installed
2024-03-15 10:03:53,594:INFO:             fastapi: Not installed
2024-03-15 10:03:53,594:INFO:             uvicorn: Not installed
2024-03-15 10:03:53,594:INFO:              m2cgen: Not installed
2024-03-15 10:03:53,594:INFO:           evidently: Not installed
2024-03-15 10:03:53,594:INFO:               fugue: Not installed
2024-03-15 10:03:53,594:INFO:           streamlit: Not installed
2024-03-15 10:03:53,594:INFO:             prophet: Not installed
2024-03-15 10:03:53,594:INFO:None
2024-03-15 10:03:53,594:INFO:Set up data.
2024-03-15 10:03:53,610:INFO:Set up folding strategy.
2024-03-15 10:03:53,610:INFO:Set up train/test split.
2024-03-15 10:03:53,626:INFO:Set up index.
2024-03-15 10:03:53,626:INFO:Assigning column types.
2024-03-15 10:03:53,626:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-03-15 10:03:53,657:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-03-15 10:03:53,665:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-03-15 10:03:53,689:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,540:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,573:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-03-15 10:03:54,574:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-03-15 10:03:54,596:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,596:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-03-15 10:03:54,630:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-03-15 10:03:54,651:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,682:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-03-15 10:03:54,698:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,705:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,705:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-03-15 10:03:54,758:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,762:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,817:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:54,817:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:54,830:INFO:Preparing preprocessing pipeline...
2024-03-15 10:03:54,832:INFO:Set up simple imputation.
2024-03-15 10:03:54,832:INFO:Set up column name cleaning.
2024-03-15 10:03:54,858:INFO:Finished creating preprocessing pipeline.
2024-03-15 10:03:54,865:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\salvi\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0', 'Glucose',
                                             'Cholesterol', 'Hemoglobin',
                                             'Platelets', 'White Blood Cells',
                                             'Red Blood Cells', 'Hematocrit',
                                             'Mean Corpuscular Volume',
                                             'Mean Corpuscular Hemoglobin',
                                             'Mean Corpuscular Hemoglobin '
                                             'Concentration',...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-03-15 10:03:54,865:INFO:Creating final display dataframe.
2024-03-15 10:03:54,956:INFO:Setup _display_container:                     Description              Value
0                    Session id               2477
1                        Target            Disease
2                   Target type         Multiclass
3           Original data shape         (2351, 26)
4        Transformed data shape         (2351, 26)
5   Transformed train set shape         (1645, 26)
6    Transformed test set shape          (706, 26)
7              Numeric features                 25
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12               Fold Generator    StratifiedKFold
13                  Fold Number                 10
14                     CPU Jobs                 -1
15                      Use GPU              False
16               Log Experiment              False
17              Experiment Name  DiseasePrediction
18                          USI               3ce0
2024-03-15 10:03:55,017:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:55,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:55,059:INFO:Soft dependency imported: xgboost: 1.7.3
2024-03-15 10:03:55,067:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-03-15 10:03:55,071:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:51: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.
  warnings.warn(

2024-03-15 10:03:55,073:INFO:setup() successfully completed in 1.6s...............
2024-03-15 10:04:14,701:INFO:Initializing compare_models()
2024-03-15 10:04:14,701:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-03-15 10:04:14,702:INFO:Checking exceptions
2024-03-15 10:04:14,710:INFO:Preparing display monitor
2024-03-15 10:04:14,747:INFO:Initializing Logistic Regression
2024-03-15 10:04:14,747:INFO:Total runtime is 0.0 minutes
2024-03-15 10:04:14,755:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:14,756:INFO:Initializing create_model()
2024-03-15 10:04:14,756:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:14,756:INFO:Checking exceptions
2024-03-15 10:04:14,756:INFO:Importing libraries
2024-03-15 10:04:14,756:INFO:Copying training dataset
2024-03-15 10:04:14,759:INFO:Defining folds
2024-03-15 10:04:14,759:INFO:Declaring metric variables
2024-03-15 10:04:14,764:INFO:Importing untrained model
2024-03-15 10:04:14,767:INFO:Logistic Regression Imported successfully
2024-03-15 10:04:14,774:INFO:Starting cross validation
2024-03-15 10:04:14,778:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:22,654:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,654:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,678:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,678:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,678:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,686:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,686:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,686:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(


  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,704:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,705:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,705:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,711:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,719:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,720:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,722:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,730:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-03-15 10:04:22,739:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,754:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,760:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:22,786:INFO:Calculating mean and std
2024-03-15 10:04:22,786:INFO:Creating metrics dataframe
2024-03-15 10:04:22,790:INFO:Uploading results into container
2024-03-15 10:04:22,790:INFO:Uploading model into container now
2024-03-15 10:04:22,790:INFO:_master_model_container: 1
2024-03-15 10:04:22,790:INFO:_display_container: 2
2024-03-15 10:04:22,800:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2477, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-03-15 10:04:22,800:INFO:create_model() successfully completed......................................
2024-03-15 10:04:22,921:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:22,921:INFO:Creating metrics dataframe
2024-03-15 10:04:22,936:INFO:Initializing K Neighbors Classifier
2024-03-15 10:04:22,938:INFO:Total runtime is 0.1365228533744812 minutes
2024-03-15 10:04:22,941:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:22,941:INFO:Initializing create_model()
2024-03-15 10:04:22,941:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:22,941:INFO:Checking exceptions
2024-03-15 10:04:22,941:INFO:Importing libraries
2024-03-15 10:04:22,941:INFO:Copying training dataset
2024-03-15 10:04:22,946:INFO:Defining folds
2024-03-15 10:04:22,946:INFO:Declaring metric variables
2024-03-15 10:04:22,955:INFO:Importing untrained model
2024-03-15 10:04:22,961:INFO:K Neighbors Classifier Imported successfully
2024-03-15 10:04:22,969:INFO:Starting cross validation
2024-03-15 10:04:22,969:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:23,205:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,205:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,205:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,205:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,205:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,211:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:23,211:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,487:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,503:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:26,518:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,535:INFO:Calculating mean and std
2024-03-15 10:04:26,535:INFO:Creating metrics dataframe
2024-03-15 10:04:26,535:INFO:Uploading results into container
2024-03-15 10:04:26,535:INFO:Uploading model into container now
2024-03-15 10:04:26,535:INFO:_master_model_container: 2
2024-03-15 10:04:26,535:INFO:_display_container: 2
2024-03-15 10:04:26,535:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-03-15 10:04:26,535:INFO:create_model() successfully completed......................................
2024-03-15 10:04:26,614:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:26,614:INFO:Creating metrics dataframe
2024-03-15 10:04:26,628:INFO:Initializing Naive Bayes
2024-03-15 10:04:26,628:INFO:Total runtime is 0.19802785714467366 minutes
2024-03-15 10:04:26,628:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:26,628:INFO:Initializing create_model()
2024-03-15 10:04:26,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:26,628:INFO:Checking exceptions
2024-03-15 10:04:26,628:INFO:Importing libraries
2024-03-15 10:04:26,628:INFO:Copying training dataset
2024-03-15 10:04:26,633:INFO:Defining folds
2024-03-15 10:04:26,633:INFO:Declaring metric variables
2024-03-15 10:04:26,633:INFO:Importing untrained model
2024-03-15 10:04:26,640:INFO:Naive Bayes Imported successfully
2024-03-15 10:04:26,646:INFO:Starting cross validation
2024-03-15 10:04:26,652:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:26,708:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,708:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,708:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,708:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,715:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,715:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,715:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,715:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,715:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,734:INFO:Calculating mean and std
2024-03-15 10:04:26,734:INFO:Creating metrics dataframe
2024-03-15 10:04:26,735:INFO:Uploading results into container
2024-03-15 10:04:26,735:INFO:Uploading model into container now
2024-03-15 10:04:26,735:INFO:_master_model_container: 3
2024-03-15 10:04:26,735:INFO:_display_container: 2
2024-03-15 10:04:26,735:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-03-15 10:04:26,735:INFO:create_model() successfully completed......................................
2024-03-15 10:04:26,840:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:26,840:INFO:Creating metrics dataframe
2024-03-15 10:04:26,859:INFO:Initializing Decision Tree Classifier
2024-03-15 10:04:26,859:INFO:Total runtime is 0.2018751859664917 minutes
2024-03-15 10:04:26,861:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:26,861:INFO:Initializing create_model()
2024-03-15 10:04:26,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:26,861:INFO:Checking exceptions
2024-03-15 10:04:26,861:INFO:Importing libraries
2024-03-15 10:04:26,861:INFO:Copying training dataset
2024-03-15 10:04:26,870:INFO:Defining folds
2024-03-15 10:04:26,870:INFO:Declaring metric variables
2024-03-15 10:04:26,870:INFO:Importing untrained model
2024-03-15 10:04:26,870:INFO:Decision Tree Classifier Imported successfully
2024-03-15 10:04:26,880:INFO:Starting cross validation
2024-03-15 10:04:26,880:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,928:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,941:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,941:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,941:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:26,957:INFO:Calculating mean and std
2024-03-15 10:04:26,957:INFO:Creating metrics dataframe
2024-03-15 10:04:26,957:INFO:Uploading results into container
2024-03-15 10:04:26,957:INFO:Uploading model into container now
2024-03-15 10:04:26,957:INFO:_master_model_container: 4
2024-03-15 10:04:26,957:INFO:_display_container: 2
2024-03-15 10:04:26,957:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best')
2024-03-15 10:04:26,957:INFO:create_model() successfully completed......................................
2024-03-15 10:04:27,038:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:27,038:INFO:Creating metrics dataframe
2024-03-15 10:04:27,052:INFO:Initializing SVM - Linear Kernel
2024-03-15 10:04:27,052:INFO:Total runtime is 0.20508745908737183 minutes
2024-03-15 10:04:27,052:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:27,052:INFO:Initializing create_model()
2024-03-15 10:04:27,052:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:27,052:INFO:Checking exceptions
2024-03-15 10:04:27,052:INFO:Importing libraries
2024-03-15 10:04:27,052:INFO:Copying training dataset
2024-03-15 10:04:27,052:INFO:Defining folds
2024-03-15 10:04:27,052:INFO:Declaring metric variables
2024-03-15 10:04:27,068:INFO:Importing untrained model
2024-03-15 10:04:27,068:INFO:SVM - Linear Kernel Imported successfully
2024-03-15 10:04:27,075:INFO:Starting cross validation
2024-03-15 10:04:27,075:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:27,142:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,147:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,162:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,162:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,178:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,190:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,190:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,194:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,194:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,194:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,211:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,216:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,217:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,225:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,225:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:27,225:INFO:Calculating mean and std
2024-03-15 10:04:27,225:INFO:Creating metrics dataframe
2024-03-15 10:04:27,225:INFO:Uploading results into container
2024-03-15 10:04:27,225:INFO:Uploading model into container now
2024-03-15 10:04:27,241:INFO:_master_model_container: 5
2024-03-15 10:04:27,241:INFO:_display_container: 2
2024-03-15 10:04:27,241:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2477, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-03-15 10:04:27,241:INFO:create_model() successfully completed......................................
2024-03-15 10:04:27,304:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:27,304:INFO:Creating metrics dataframe
2024-03-15 10:04:27,320:INFO:Initializing Ridge Classifier
2024-03-15 10:04:27,320:INFO:Total runtime is 0.2095555583635966 minutes
2024-03-15 10:04:27,330:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:27,330:INFO:Initializing create_model()
2024-03-15 10:04:27,330:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:27,330:INFO:Checking exceptions
2024-03-15 10:04:27,330:INFO:Importing libraries
2024-03-15 10:04:27,330:INFO:Copying training dataset
2024-03-15 10:04:27,338:INFO:Defining folds
2024-03-15 10:04:27,338:INFO:Declaring metric variables
2024-03-15 10:04:27,341:INFO:Importing untrained model
2024-03-15 10:04:27,344:INFO:Ridge Classifier Imported successfully
2024-03-15 10:04:27,351:INFO:Starting cross validation
2024-03-15 10:04:27,351:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:27,411:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,411:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,421:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,421:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,421:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,425:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,425:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,425:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,432:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-03-15 10:04:27,445:INFO:Calculating mean and std
2024-03-15 10:04:27,445:INFO:Creating metrics dataframe
2024-03-15 10:04:27,445:INFO:Uploading results into container
2024-03-15 10:04:27,445:INFO:Uploading model into container now
2024-03-15 10:04:27,445:INFO:_master_model_container: 6
2024-03-15 10:04:27,445:INFO:_display_container: 2
2024-03-15 10:04:27,445:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2477, solver='auto',
                tol=0.0001)
2024-03-15 10:04:27,445:INFO:create_model() successfully completed......................................
2024-03-15 10:04:27,514:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:27,514:INFO:Creating metrics dataframe
2024-03-15 10:04:27,528:INFO:Initializing Random Forest Classifier
2024-03-15 10:04:27,528:INFO:Total runtime is 0.21301814715067546 minutes
2024-03-15 10:04:27,528:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:27,528:INFO:Initializing create_model()
2024-03-15 10:04:27,528:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:27,528:INFO:Checking exceptions
2024-03-15 10:04:27,528:INFO:Importing libraries
2024-03-15 10:04:27,528:INFO:Copying training dataset
2024-03-15 10:04:27,528:INFO:Defining folds
2024-03-15 10:04:27,528:INFO:Declaring metric variables
2024-03-15 10:04:27,540:INFO:Importing untrained model
2024-03-15 10:04:27,544:INFO:Random Forest Classifier Imported successfully
2024-03-15 10:04:27,544:INFO:Starting cross validation
2024-03-15 10:04:27,544:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:27,847:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:27,850:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:27,850:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:27,855:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:27,858:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:27,900:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,019:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,024:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,052:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,086:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,101:INFO:Calculating mean and std
2024-03-15 10:04:28,101:INFO:Creating metrics dataframe
2024-03-15 10:04:28,101:INFO:Uploading results into container
2024-03-15 10:04:28,101:INFO:Uploading model into container now
2024-03-15 10:04:28,101:INFO:_master_model_container: 7
2024-03-15 10:04:28,101:INFO:_display_container: 2
2024-03-15 10:04:28,101:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2477, verbose=0,
                       warm_start=False)
2024-03-15 10:04:28,101:INFO:create_model() successfully completed......................................
2024-03-15 10:04:28,175:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:28,175:INFO:Creating metrics dataframe
2024-03-15 10:04:28,190:INFO:Initializing Quadratic Discriminant Analysis
2024-03-15 10:04:28,190:INFO:Total runtime is 0.2240565578142802 minutes
2024-03-15 10:04:28,190:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:28,190:INFO:Initializing create_model()
2024-03-15 10:04:28,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:28,190:INFO:Checking exceptions
2024-03-15 10:04:28,190:INFO:Importing libraries
2024-03-15 10:04:28,190:INFO:Copying training dataset
2024-03-15 10:04:28,206:INFO:Defining folds
2024-03-15 10:04:28,206:INFO:Declaring metric variables
2024-03-15 10:04:28,206:INFO:Importing untrained model
2024-03-15 10:04:28,206:INFO:Quadratic Discriminant Analysis Imported successfully
2024-03-15 10:04:28,217:INFO:Starting cross validation
2024-03-15 10:04:28,217:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:28,268:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,268:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,268:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,268:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,268:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,276:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,276:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,284:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-03-15 10:04:28,300:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,300:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,300:INFO:Calculating mean and std
2024-03-15 10:04:28,300:INFO:Creating metrics dataframe
2024-03-15 10:04:28,300:INFO:Uploading results into container
2024-03-15 10:04:28,300:INFO:Uploading model into container now
2024-03-15 10:04:28,316:INFO:_master_model_container: 8
2024-03-15 10:04:28,316:INFO:_display_container: 2
2024-03-15 10:04:28,316:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-03-15 10:04:28,316:INFO:create_model() successfully completed......................................
2024-03-15 10:04:28,380:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:28,380:INFO:Creating metrics dataframe
2024-03-15 10:04:28,394:INFO:Initializing Ada Boost Classifier
2024-03-15 10:04:28,394:INFO:Total runtime is 0.2274630586306254 minutes
2024-03-15 10:04:28,394:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:28,394:INFO:Initializing create_model()
2024-03-15 10:04:28,394:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:28,394:INFO:Checking exceptions
2024-03-15 10:04:28,394:INFO:Importing libraries
2024-03-15 10:04:28,394:INFO:Copying training dataset
2024-03-15 10:04:28,394:INFO:Defining folds
2024-03-15 10:04:28,394:INFO:Declaring metric variables
2024-03-15 10:04:28,394:INFO:Importing untrained model
2024-03-15 10:04:28,410:INFO:Ada Boost Classifier Imported successfully
2024-03-15 10:04:28,417:INFO:Starting cross validation
2024-03-15 10:04:28,417:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,442:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,458:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,458:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-03-15 10:04:28,630:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,646:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,646:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,661:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,661:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,661:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,677:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,677:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,693:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,693:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:28,693:INFO:Calculating mean and std
2024-03-15 10:04:28,693:INFO:Creating metrics dataframe
2024-03-15 10:04:28,708:INFO:Uploading results into container
2024-03-15 10:04:28,708:INFO:Uploading model into container now
2024-03-15 10:04:28,708:INFO:_master_model_container: 9
2024-03-15 10:04:28,708:INFO:_display_container: 2
2024-03-15 10:04:28,708:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2477)
2024-03-15 10:04:28,708:INFO:create_model() successfully completed......................................
2024-03-15 10:04:28,788:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:28,788:INFO:Creating metrics dataframe
2024-03-15 10:04:28,788:INFO:Initializing Gradient Boosting Classifier
2024-03-15 10:04:28,788:INFO:Total runtime is 0.2340144713719686 minutes
2024-03-15 10:04:28,788:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:28,788:INFO:Initializing create_model()
2024-03-15 10:04:28,788:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:28,788:INFO:Checking exceptions
2024-03-15 10:04:28,788:INFO:Importing libraries
2024-03-15 10:04:28,788:INFO:Copying training dataset
2024-03-15 10:04:28,804:INFO:Defining folds
2024-03-15 10:04:28,804:INFO:Declaring metric variables
2024-03-15 10:04:28,804:INFO:Importing untrained model
2024-03-15 10:04:28,814:INFO:Gradient Boosting Classifier Imported successfully
2024-03-15 10:04:28,818:INFO:Starting cross validation
2024-03-15 10:04:28,821:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:31,088:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,103:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,119:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,135:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,150:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,166:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,182:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,182:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,182:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,275:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,291:INFO:Calculating mean and std
2024-03-15 10:04:31,293:INFO:Creating metrics dataframe
2024-03-15 10:04:31,299:INFO:Uploading results into container
2024-03-15 10:04:31,299:INFO:Uploading model into container now
2024-03-15 10:04:31,299:INFO:_master_model_container: 10
2024-03-15 10:04:31,299:INFO:_display_container: 2
2024-03-15 10:04:31,299:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2477, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-03-15 10:04:31,299:INFO:create_model() successfully completed......................................
2024-03-15 10:04:31,370:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:31,370:INFO:Creating metrics dataframe
2024-03-15 10:04:31,386:INFO:Initializing Linear Discriminant Analysis
2024-03-15 10:04:31,386:INFO:Total runtime is 0.2773193597793579 minutes
2024-03-15 10:04:31,386:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:31,386:INFO:Initializing create_model()
2024-03-15 10:04:31,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:31,386:INFO:Checking exceptions
2024-03-15 10:04:31,386:INFO:Importing libraries
2024-03-15 10:04:31,386:INFO:Copying training dataset
2024-03-15 10:04:31,386:INFO:Defining folds
2024-03-15 10:04:31,386:INFO:Declaring metric variables
2024-03-15 10:04:31,401:INFO:Importing untrained model
2024-03-15 10:04:31,401:INFO:Linear Discriminant Analysis Imported successfully
2024-03-15 10:04:31,417:INFO:Starting cross validation
2024-03-15 10:04:31,418:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:31,467:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,467:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,467:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,467:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,467:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,480:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,480:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,480:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,480:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,496:INFO:Calculating mean and std
2024-03-15 10:04:31,496:INFO:Creating metrics dataframe
2024-03-15 10:04:31,496:INFO:Uploading results into container
2024-03-15 10:04:31,496:INFO:Uploading model into container now
2024-03-15 10:04:31,496:INFO:_master_model_container: 11
2024-03-15 10:04:31,496:INFO:_display_container: 2
2024-03-15 10:04:31,496:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-03-15 10:04:31,496:INFO:create_model() successfully completed......................................
2024-03-15 10:04:31,591:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:31,591:INFO:Creating metrics dataframe
2024-03-15 10:04:31,591:INFO:Initializing Extra Trees Classifier
2024-03-15 10:04:31,591:INFO:Total runtime is 0.28074362277984616 minutes
2024-03-15 10:04:31,591:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:31,591:INFO:Initializing create_model()
2024-03-15 10:04:31,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:31,591:INFO:Checking exceptions
2024-03-15 10:04:31,591:INFO:Importing libraries
2024-03-15 10:04:31,591:INFO:Copying training dataset
2024-03-15 10:04:31,610:INFO:Defining folds
2024-03-15 10:04:31,610:INFO:Declaring metric variables
2024-03-15 10:04:31,610:INFO:Importing untrained model
2024-03-15 10:04:31,616:INFO:Extra Trees Classifier Imported successfully
2024-03-15 10:04:31,621:INFO:Starting cross validation
2024-03-15 10:04:31,622:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:31,860:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,860:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,875:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,875:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,875:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,875:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,891:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,891:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,891:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,969:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:31,985:INFO:Calculating mean and std
2024-03-15 10:04:31,985:INFO:Creating metrics dataframe
2024-03-15 10:04:31,985:INFO:Uploading results into container
2024-03-15 10:04:31,985:INFO:Uploading model into container now
2024-03-15 10:04:31,985:INFO:_master_model_container: 12
2024-03-15 10:04:31,985:INFO:_display_container: 2
2024-03-15 10:04:31,985:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2477, verbose=0,
                     warm_start=False)
2024-03-15 10:04:31,985:INFO:create_model() successfully completed......................................
2024-03-15 10:04:32,064:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:32,064:INFO:Creating metrics dataframe
2024-03-15 10:04:32,064:INFO:Initializing Extreme Gradient Boosting
2024-03-15 10:04:32,064:INFO:Total runtime is 0.2886152148246765 minutes
2024-03-15 10:04:32,079:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:32,079:INFO:Initializing create_model()
2024-03-15 10:04:32,079:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:32,079:INFO:Checking exceptions
2024-03-15 10:04:32,079:INFO:Importing libraries
2024-03-15 10:04:32,079:INFO:Copying training dataset
2024-03-15 10:04:32,083:INFO:Defining folds
2024-03-15 10:04:32,083:INFO:Declaring metric variables
2024-03-15 10:04:32,088:INFO:Importing untrained model
2024-03-15 10:04:32,088:INFO:Extreme Gradient Boosting Imported successfully
2024-03-15 10:04:32,095:INFO:Starting cross validation
2024-03-15 10:04:32,095:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:32,581:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,612:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,612:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,628:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,643:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:32,643:INFO:Calculating mean and std
2024-03-15 10:04:32,659:INFO:Creating metrics dataframe
2024-03-15 10:04:32,659:INFO:Uploading results into container
2024-03-15 10:04:32,659:INFO:Uploading model into container now
2024-03-15 10:04:32,659:INFO:_master_model_container: 13
2024-03-15 10:04:32,659:INFO:_display_container: 2
2024-03-15 10:04:32,659:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2024-03-15 10:04:32,659:INFO:create_model() successfully completed......................................
2024-03-15 10:04:32,722:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:32,722:INFO:Creating metrics dataframe
2024-03-15 10:04:32,737:INFO:Initializing Light Gradient Boosting Machine
2024-03-15 10:04:32,737:INFO:Total runtime is 0.299843962987264 minutes
2024-03-15 10:04:32,737:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:32,737:INFO:Initializing create_model()
2024-03-15 10:04:32,748:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:32,748:INFO:Checking exceptions
2024-03-15 10:04:32,748:INFO:Importing libraries
2024-03-15 10:04:32,748:INFO:Copying training dataset
2024-03-15 10:04:32,753:INFO:Defining folds
2024-03-15 10:04:32,753:INFO:Declaring metric variables
2024-03-15 10:04:32,753:INFO:Importing untrained model
2024-03-15 10:04:32,753:INFO:Light Gradient Boosting Machine Imported successfully
2024-03-15 10:04:32,770:INFO:Starting cross validation
2024-03-15 10:04:32,771:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:34,093:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,093:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,109:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,109:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,140:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,140:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,140:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,140:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,171:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,171:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,171:INFO:Calculating mean and std
2024-03-15 10:04:34,171:INFO:Creating metrics dataframe
2024-03-15 10:04:34,187:INFO:Uploading results into container
2024-03-15 10:04:34,187:INFO:Uploading model into container now
2024-03-15 10:04:34,187:INFO:_master_model_container: 14
2024-03-15 10:04:34,187:INFO:_display_container: 2
2024-03-15 10:04:34,187:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2477, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2024-03-15 10:04:34,187:INFO:create_model() successfully completed......................................
2024-03-15 10:04:34,266:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:34,266:INFO:Creating metrics dataframe
2024-03-15 10:04:34,281:INFO:Initializing Dummy Classifier
2024-03-15 10:04:34,282:INFO:Total runtime is 0.3255914568901062 minutes
2024-03-15 10:04:34,282:INFO:SubProcess create_model() called ==================================
2024-03-15 10:04:34,282:INFO:Initializing create_model()
2024-03-15 10:04:34,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000257EFFA0E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:34,282:INFO:Checking exceptions
2024-03-15 10:04:34,282:INFO:Importing libraries
2024-03-15 10:04:34,282:INFO:Copying training dataset
2024-03-15 10:04:34,289:INFO:Defining folds
2024-03-15 10:04:34,289:INFO:Declaring metric variables
2024-03-15 10:04:34,291:INFO:Importing untrained model
2024-03-15 10:04:34,298:INFO:Dummy Classifier Imported successfully
2024-03-15 10:04:34,298:INFO:Starting cross validation
2024-03-15 10:04:34,298:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 10:04:34,329:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,329:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,329:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,329:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,329:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,345:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,360:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-03-15 10:04:34,376:INFO:Calculating mean and std
2024-03-15 10:04:34,376:INFO:Creating metrics dataframe
2024-03-15 10:04:34,376:INFO:Uploading results into container
2024-03-15 10:04:34,376:INFO:Uploading model into container now
2024-03-15 10:04:34,376:INFO:_master_model_container: 15
2024-03-15 10:04:34,376:INFO:_display_container: 2
2024-03-15 10:04:34,376:INFO:DummyClassifier(constant=None, random_state=2477, strategy='prior')
2024-03-15 10:04:34,376:INFO:create_model() successfully completed......................................
2024-03-15 10:04:34,455:INFO:SubProcess create_model() end ==================================
2024-03-15 10:04:34,455:INFO:Creating metrics dataframe
2024-03-15 10:04:34,487:INFO:Initializing create_model()
2024-03-15 10:04:34,487:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:04:34,487:INFO:Checking exceptions
2024-03-15 10:04:34,487:INFO:Importing libraries
2024-03-15 10:04:34,487:INFO:Copying training dataset
2024-03-15 10:04:34,502:INFO:Defining folds
2024-03-15 10:04:34,502:INFO:Declaring metric variables
2024-03-15 10:04:34,502:INFO:Importing untrained model
2024-03-15 10:04:34,502:INFO:Declaring custom model
2024-03-15 10:04:34,502:INFO:Decision Tree Classifier Imported successfully
2024-03-15 10:04:34,502:INFO:Cross validation set to False
2024-03-15 10:04:34,502:INFO:Fitting Model
2024-03-15 10:04:34,518:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best')
2024-03-15 10:04:34,518:INFO:create_model() successfully completed......................................
2024-03-15 10:04:34,644:INFO:_master_model_container: 15
2024-03-15 10:04:34,644:INFO:_display_container: 2
2024-03-15 10:04:34,644:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best')
2024-03-15 10:04:34,644:INFO:compare_models() successfully completed......................................
2024-03-15 10:47:35,626:INFO:Initializing evaluate_model()
2024-03-15 10:47:35,626:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-03-15 10:47:35,641:INFO:Initializing plot_model()
2024-03-15 10:47:35,641:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:47:35,641:INFO:Checking exceptions
2024-03-15 10:47:35,643:INFO:Preloading libraries
2024-03-15 10:47:35,643:INFO:Copying training dataset
2024-03-15 10:47:35,643:INFO:Plot type: pipeline
2024-03-15 10:47:35,828:INFO:Visual Rendered Successfully
2024-03-15 10:47:35,922:INFO:plot_model() successfully completed......................................
2024-03-15 10:47:53,008:INFO:Initializing plot_model()
2024-03-15 10:47:53,008:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:47:53,008:INFO:Checking exceptions
2024-03-15 10:47:53,014:INFO:Preloading libraries
2024-03-15 10:47:53,014:INFO:Copying training dataset
2024-03-15 10:47:53,014:INFO:Plot type: parameter
2024-03-15 10:47:53,017:INFO:Visual Rendered Successfully
2024-03-15 10:47:53,092:INFO:plot_model() successfully completed......................................
2024-03-15 10:47:55,662:INFO:Initializing plot_model()
2024-03-15 10:47:55,662:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:47:55,662:INFO:Checking exceptions
2024-03-15 10:47:55,662:INFO:Preloading libraries
2024-03-15 10:47:55,662:INFO:Copying training dataset
2024-03-15 10:47:55,662:INFO:Plot type: pipeline
2024-03-15 10:47:55,736:INFO:Visual Rendered Successfully
2024-03-15 10:47:55,829:INFO:plot_model() successfully completed......................................
2024-03-15 10:47:57,347:INFO:Initializing plot_model()
2024-03-15 10:47:57,347:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:47:57,347:INFO:Checking exceptions
2024-03-15 10:47:57,355:INFO:Preloading libraries
2024-03-15 10:47:57,355:INFO:Copying training dataset
2024-03-15 10:47:57,355:INFO:Plot type: error
2024-03-15 10:47:57,405:INFO:Fitting Model
2024-03-15 10:47:57,443:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
  warnings.warn(

2024-03-15 10:47:57,443:INFO:Scoring test/hold-out set
2024-03-15 10:47:57,642:INFO:Visual Rendered Successfully
2024-03-15 10:47:57,729:INFO:plot_model() successfully completed......................................
2024-03-15 10:47:59,950:INFO:Initializing plot_model()
2024-03-15 10:47:59,952:INFO:plot_model(plot=vc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:47:59,952:INFO:Checking exceptions
2024-03-15 10:47:59,952:INFO:Preloading libraries
2024-03-15 10:47:59,952:INFO:Copying training dataset
2024-03-15 10:47:59,952:INFO:Plot type: vc
2024-03-15 10:47:59,952:INFO:Determining param_name
2024-03-15 10:47:59,952:INFO:param_name: max_depth
2024-03-15 10:48:00,066:INFO:Fitting Model
2024-03-15 10:48:06,123:INFO:Visual Rendered Successfully
2024-03-15 10:48:06,215:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:06,243:INFO:Initializing plot_model()
2024-03-15 10:48:06,243:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:06,243:INFO:Checking exceptions
2024-03-15 10:48:06,245:INFO:Preloading libraries
2024-03-15 10:48:06,245:INFO:Copying training dataset
2024-03-15 10:48:06,245:INFO:Plot type: pipeline
2024-03-15 10:48:06,389:INFO:Visual Rendered Successfully
2024-03-15 10:48:06,498:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:07,517:INFO:Initializing plot_model()
2024-03-15 10:48:07,517:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:07,517:INFO:Checking exceptions
2024-03-15 10:48:07,520:INFO:Preloading libraries
2024-03-15 10:48:07,520:INFO:Copying training dataset
2024-03-15 10:48:07,520:INFO:Plot type: parameter
2024-03-15 10:48:07,520:INFO:Visual Rendered Successfully
2024-03-15 10:48:07,602:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:09,223:INFO:Initializing plot_model()
2024-03-15 10:48:09,223:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:09,223:INFO:Checking exceptions
2024-03-15 10:48:09,224:INFO:Preloading libraries
2024-03-15 10:48:09,224:INFO:Copying training dataset
2024-03-15 10:48:09,224:INFO:Plot type: pipeline
2024-03-15 10:48:09,292:INFO:Visual Rendered Successfully
2024-03-15 10:48:09,389:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:16,444:INFO:Initializing plot_model()
2024-03-15 10:48:16,444:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:16,444:INFO:Checking exceptions
2024-03-15 10:48:16,446:INFO:Preloading libraries
2024-03-15 10:48:16,446:INFO:Copying training dataset
2024-03-15 10:48:16,446:INFO:Plot type: tree
2024-03-15 10:48:16,461:INFO:Plotting decision trees
2024-03-15 10:48:23,701:INFO:Initializing plot_model()
2024-03-15 10:48:23,701:INFO:plot_model(plot=dimension, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:23,701:INFO:Checking exceptions
2024-03-15 10:48:23,704:INFO:Preloading libraries
2024-03-15 10:48:23,704:INFO:Copying training dataset
2024-03-15 10:48:23,704:INFO:Plot type: dimension
2024-03-15 10:48:23,774:INFO:Fitting StandardScaler()
2024-03-15 10:48:23,790:INFO:Fitting PCA()
2024-03-15 10:48:23,947:INFO:Fitting & Transforming Model
2024-03-15 10:48:24,125:INFO:Visual Rendered Successfully
2024-03-15 10:48:24,247:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:26,858:INFO:Initializing plot_model()
2024-03-15 10:48:26,858:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:26,858:INFO:Checking exceptions
2024-03-15 10:48:26,866:INFO:Preloading libraries
2024-03-15 10:48:26,866:INFO:Copying training dataset
2024-03-15 10:48:26,866:INFO:Plot type: parameter
2024-03-15 10:48:26,866:INFO:Visual Rendered Successfully
2024-03-15 10:48:26,967:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:40,514:INFO:Initializing plot_model()
2024-03-15 10:48:40,514:INFO:plot_model(plot=ks, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:40,514:INFO:Checking exceptions
2024-03-15 10:48:40,522:INFO:Preloading libraries
2024-03-15 10:48:40,522:INFO:Copying training dataset
2024-03-15 10:48:40,522:INFO:Plot type: ks
2024-03-15 10:48:40,522:INFO:Generating predictions / predict_proba on X_test
2024-03-15 10:48:41,712:INFO:Initializing plot_model()
2024-03-15 10:48:41,712:INFO:plot_model(plot=feature, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:41,712:INFO:Checking exceptions
2024-03-15 10:48:41,713:INFO:Preloading libraries
2024-03-15 10:48:41,713:INFO:Copying training dataset
2024-03-15 10:48:41,713:INFO:Plot type: feature
2024-03-15 10:48:41,713:WARNING:No coef_ found. Trying feature_importances_
2024-03-15 10:48:41,840:INFO:Visual Rendered Successfully
2024-03-15 10:48:41,935:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:45,038:INFO:Initializing plot_model()
2024-03-15 10:48:45,038:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:45,038:INFO:Checking exceptions
2024-03-15 10:48:45,038:INFO:Preloading libraries
2024-03-15 10:48:45,038:INFO:Copying training dataset
2024-03-15 10:48:45,038:INFO:Plot type: confusion_matrix
2024-03-15 10:48:45,095:INFO:Fitting Model
2024-03-15 10:48:45,095:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
  warnings.warn(

2024-03-15 10:48:45,095:INFO:Scoring test/hold-out set
2024-03-15 10:48:45,221:INFO:Visual Rendered Successfully
2024-03-15 10:48:45,301:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:52,830:INFO:Initializing plot_model()
2024-03-15 10:48:52,831:INFO:plot_model(plot=manifold, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:52,831:INFO:Checking exceptions
2024-03-15 10:48:53,906:INFO:Initializing plot_model()
2024-03-15 10:48:53,906:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:53,906:INFO:Checking exceptions
2024-03-15 10:48:53,907:INFO:Preloading libraries
2024-03-15 10:48:53,907:INFO:Copying training dataset
2024-03-15 10:48:53,907:INFO:Plot type: boundary
2024-03-15 10:48:53,964:INFO:Fitting StandardScaler()
2024-03-15 10:48:53,964:INFO:Fitting PCA()
2024-03-15 10:48:54,105:INFO:Fitting Model
2024-03-15 10:48:55,001:INFO:Visual Rendered Successfully
2024-03-15 10:48:55,147:INFO:plot_model() successfully completed......................................
2024-03-15 10:48:57,471:INFO:Initializing plot_model()
2024-03-15 10:48:57,471:INFO:plot_model(plot=calibration, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:57,471:INFO:Checking exceptions
2024-03-15 10:48:58,362:INFO:Initializing plot_model()
2024-03-15 10:48:58,362:INFO:plot_model(plot=pr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:48:58,362:INFO:Checking exceptions
2024-03-15 10:48:58,371:INFO:Preloading libraries
2024-03-15 10:48:58,371:INFO:Copying training dataset
2024-03-15 10:48:58,371:INFO:Plot type: pr
2024-03-15 10:48:58,435:INFO:Fitting Model
2024-03-15 10:48:58,479:INFO:Scoring test/hold-out set
2024-03-15 10:48:58,635:INFO:Visual Rendered Successfully
2024-03-15 10:48:58,730:INFO:plot_model() successfully completed......................................
2024-03-15 10:49:02,139:INFO:Initializing plot_model()
2024-03-15 10:49:02,139:INFO:plot_model(plot=lift, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:02,139:INFO:Checking exceptions
2024-03-15 10:49:02,139:INFO:Preloading libraries
2024-03-15 10:49:02,139:INFO:Copying training dataset
2024-03-15 10:49:02,139:INFO:Plot type: lift
2024-03-15 10:49:02,139:INFO:Generating predictions / predict_proba on X_test
2024-03-15 10:49:03,879:INFO:Initializing plot_model()
2024-03-15 10:49:03,883:INFO:plot_model(plot=calibration, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:03,883:INFO:Checking exceptions
2024-03-15 10:49:05,214:INFO:Initializing plot_model()
2024-03-15 10:49:05,214:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:05,214:INFO:Checking exceptions
2024-03-15 10:49:05,222:INFO:Preloading libraries
2024-03-15 10:49:05,222:INFO:Copying training dataset
2024-03-15 10:49:05,222:INFO:Plot type: pipeline
2024-03-15 10:49:05,288:INFO:Visual Rendered Successfully
2024-03-15 10:49:05,367:INFO:plot_model() successfully completed......................................
2024-03-15 10:49:06,125:INFO:Initializing plot_model()
2024-03-15 10:49:06,125:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:06,125:INFO:Checking exceptions
2024-03-15 10:49:06,127:INFO:Preloading libraries
2024-03-15 10:49:06,127:INFO:Copying training dataset
2024-03-15 10:49:06,127:INFO:Plot type: error
2024-03-15 10:49:06,193:INFO:Fitting Model
2024-03-15 10:49:06,194:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
  warnings.warn(

2024-03-15 10:49:06,194:INFO:Scoring test/hold-out set
2024-03-15 10:49:06,359:INFO:Visual Rendered Successfully
2024-03-15 10:49:06,452:INFO:plot_model() successfully completed......................................
2024-03-15 10:49:07,558:INFO:Initializing plot_model()
2024-03-15 10:49:07,562:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:07,562:INFO:Checking exceptions
2024-03-15 10:49:07,563:INFO:Preloading libraries
2024-03-15 10:49:07,563:INFO:Copying training dataset
2024-03-15 10:49:07,563:INFO:Plot type: gain
2024-03-15 10:49:07,563:INFO:Generating predictions / predict_proba on X_test
2024-03-15 10:49:08,651:INFO:Initializing plot_model()
2024-03-15 10:49:08,651:INFO:plot_model(plot=vc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:49:08,651:INFO:Checking exceptions
2024-03-15 10:49:08,654:INFO:Preloading libraries
2024-03-15 10:49:08,654:INFO:Copying training dataset
2024-03-15 10:49:08,654:INFO:Plot type: vc
2024-03-15 10:49:08,654:INFO:Determining param_name
2024-03-15 10:49:08,654:INFO:param_name: max_depth
2024-03-15 10:49:08,728:INFO:Fitting Model
2024-03-15 10:49:09,095:INFO:Visual Rendered Successfully
2024-03-15 10:49:09,174:INFO:plot_model() successfully completed......................................
2024-03-15 10:52:57,963:INFO:Initializing plot_model()
2024-03-15 10:52:57,963:INFO:plot_model(plot=dimension, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:52:57,963:INFO:Checking exceptions
2024-03-15 10:52:57,965:INFO:Preloading libraries
2024-03-15 10:52:57,989:INFO:Copying training dataset
2024-03-15 10:52:57,989:INFO:Plot type: dimension
2024-03-15 10:52:57,997:INFO:Fitting StandardScaler()
2024-03-15 10:52:58,028:INFO:Fitting PCA()
2024-03-15 10:52:58,186:INFO:Fitting & Transforming Model
2024-03-15 10:52:58,336:INFO:Visual Rendered Successfully
2024-03-15 10:52:58,439:INFO:plot_model() successfully completed......................................
2024-03-15 10:52:59,995:INFO:Initializing plot_model()
2024-03-15 10:52:59,995:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:52:59,995:INFO:Checking exceptions
2024-03-15 10:53:00,003:INFO:Preloading libraries
2024-03-15 10:53:00,003:INFO:Copying training dataset
2024-03-15 10:53:00,003:INFO:Plot type: tree
2024-03-15 10:53:00,015:INFO:Plotting decision trees
2024-03-15 10:53:17,121:INFO:Initializing plot_model()
2024-03-15 10:53:17,121:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:53:17,121:INFO:Checking exceptions
2024-03-15 10:53:17,121:INFO:Preloading libraries
2024-03-15 10:53:17,121:INFO:Copying training dataset
2024-03-15 10:53:17,121:INFO:Plot type: pipeline
2024-03-15 10:53:17,184:INFO:Visual Rendered Successfully
2024-03-15 10:53:17,284:INFO:plot_model() successfully completed......................................
2024-03-15 10:53:26,454:INFO:Initializing plot_model()
2024-03-15 10:53:26,455:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:53:26,455:INFO:Checking exceptions
2024-03-15 10:53:26,455:INFO:Preloading libraries
2024-03-15 10:53:26,455:INFO:Copying training dataset
2024-03-15 10:53:26,455:INFO:Plot type: parameter
2024-03-15 10:53:26,455:INFO:Visual Rendered Successfully
2024-03-15 10:53:26,550:INFO:plot_model() successfully completed......................................
2024-03-15 10:53:30,135:INFO:Initializing evaluate_model()
2024-03-15 10:53:30,135:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-03-15 10:53:30,146:INFO:Initializing plot_model()
2024-03-15 10:53:30,146:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:53:30,147:INFO:Checking exceptions
2024-03-15 10:53:30,149:INFO:Preloading libraries
2024-03-15 10:53:30,149:INFO:Copying training dataset
2024-03-15 10:53:30,149:INFO:Plot type: pipeline
2024-03-15 10:53:30,220:INFO:Visual Rendered Successfully
2024-03-15 10:53:30,315:INFO:plot_model() successfully completed......................................
2024-03-15 10:53:35,129:INFO:Initializing plot_model()
2024-03-15 10:53:35,129:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, system=True)
2024-03-15 10:53:35,129:INFO:Checking exceptions
2024-03-15 10:53:35,133:INFO:Preloading libraries
2024-03-15 10:53:35,133:INFO:Copying training dataset
2024-03-15 10:53:35,134:INFO:Plot type: boundary
2024-03-15 10:53:35,158:INFO:Fitting StandardScaler()
2024-03-15 10:53:35,167:INFO:Fitting PCA()
2024-03-15 10:53:35,308:INFO:Fitting Model
2024-03-15 10:53:36,122:INFO:Visual Rendered Successfully
2024-03-15 10:53:36,256:INFO:plot_model() successfully completed......................................
2024-03-15 10:56:17,408:INFO:Initializing finalize_model()
2024-03-15 10:56:17,408:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-03-15 10:56:17,410:INFO:Finalizing DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best')
2024-03-15 10:56:17,415:INFO:Initializing create_model()
2024-03-15 10:56:17,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2477, splitter='best'), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 10:56:17,415:INFO:Checking exceptions
2024-03-15 10:56:17,417:INFO:Importing libraries
2024-03-15 10:56:17,417:INFO:Copying training dataset
2024-03-15 10:56:17,418:INFO:Defining folds
2024-03-15 10:56:17,418:INFO:Declaring metric variables
2024-03-15 10:56:17,419:INFO:Importing untrained model
2024-03-15 10:56:17,419:INFO:Declaring custom model
2024-03-15 10:56:17,419:INFO:Decision Tree Classifier Imported successfully
2024-03-15 10:56:17,420:INFO:Cross validation set to False
2024-03-15 10:56:17,420:INFO:Fitting Model
2024-03-15 10:56:17,436:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0', 'Glucose',
                                             'Cholesterol', 'Hemoglobin',
                                             'Platelets', 'White Blood Cells',
                                             'Red Blood Cells', 'Hematocrit',
                                             'Mean Corpuscular Volume',
                                             'Mean Corpuscular Hemoglobin',
                                             'Mean Corpuscular Hemoglobin '
                                             'Concentration',
                                             'Insulin', 'BMI',
                                             'Systolic Blood Pressure',...
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=None,
                                        max_features=None, max_leaf_nodes=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, random_state=2477,
                                        splitter='best'))],
         verbose=False)
2024-03-15 10:56:17,436:INFO:create_model() successfully completed......................................
2024-03-15 10:56:17,538:INFO:_master_model_container: 15
2024-03-15 10:56:17,538:INFO:_display_container: 2
2024-03-15 10:56:17,545:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0', 'Glucose',
                                             'Cholesterol', 'Hemoglobin',
                                             'Platelets', 'White Blood Cells',
                                             'Red Blood Cells', 'Hematocrit',
                                             'Mean Corpuscular Volume',
                                             'Mean Corpuscular Hemoglobin',
                                             'Mean Corpuscular Hemoglobin '
                                             'Concentration',
                                             'Insulin', 'BMI',
                                             'Systolic Blood Pressure',...
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=None,
                                        max_features=None, max_leaf_nodes=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, random_state=2477,
                                        splitter='best'))],
         verbose=False)
2024-03-15 10:56:17,545:INFO:finalize_model() successfully completed......................................
2024-03-15 10:56:44,621:INFO:Initializing predict_model()
2024-03-15 10:56:44,621:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0', 'Glucose',
                                             'Cholesterol', 'Hemoglobin',
                                             'Platelets', 'White Blood Cells',
                                             'Red Blood Cells', 'Hematocrit',
                                             'Mean Corpuscular Volume',
                                             'Mean Corpuscular Hemoglobin',
                                             'Mean Corpuscular Hemoglobin '
                                             'Concentration',
                                             'Insulin', 'BMI',
                                             'Systolic Blood Pressure',...
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=None,
                                        max_features=None, max_leaf_nodes=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, random_state=2477,
                                        splitter='best'))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000257F9505EA0>)
2024-03-15 10:56:44,621:INFO:Checking exceptions
2024-03-15 10:56:44,621:INFO:Preloading libraries
2024-03-15 10:56:44,627:INFO:Set up data.
2024-03-15 10:56:44,638:INFO:Set up index.
2024-03-15 11:09:38,476:INFO:Initializing create_model()
2024-03-15 11:09:38,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 11:09:38,476:INFO:Checking exceptions
2024-03-15 11:09:38,502:INFO:Importing libraries
2024-03-15 11:09:38,503:INFO:Copying training dataset
2024-03-15 11:09:38,511:INFO:Defining folds
2024-03-15 11:09:38,511:INFO:Declaring metric variables
2024-03-15 11:09:38,513:INFO:Importing untrained model
2024-03-15 11:09:38,522:INFO:Random Forest Classifier Imported successfully
2024-03-15 11:09:38,541:INFO:Starting cross validation
2024-03-15 11:09:38,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 11:09:46,152:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,231:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,263:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,263:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,293:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,309:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,356:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,356:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,403:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,434:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:09:46,434:INFO:Calculating mean and std
2024-03-15 11:09:46,450:INFO:Creating metrics dataframe
2024-03-15 11:09:46,452:INFO:Finalizing model
2024-03-15 11:09:46,648:INFO:Uploading results into container
2024-03-15 11:09:46,649:INFO:Uploading model into container now
2024-03-15 11:09:46,657:INFO:_master_model_container: 16
2024-03-15 11:09:46,657:INFO:_display_container: 4
2024-03-15 11:09:46,657:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2477, verbose=0,
                       warm_start=False)
2024-03-15 11:09:46,657:INFO:create_model() successfully completed......................................
2024-03-15 11:10:27,553:INFO:Initializing predict_model()
2024-03-15 11:10:27,553:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0', 'Glucose',
                                             'Cholesterol', 'Hemoglobin',
                                             'Platelets', 'White Blood Cells',
                                             'Red Blood Cells', 'Hematocrit',
                                             'Mean Corpuscular Volume',
                                             'Mean Corpuscular Hemoglobin',
                                             'Mean Corpuscular Hemoglobin '
                                             'Concentration',
                                             'Insulin', 'BMI',
                                             'Systolic Blood Pressure',...
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=None,
                                        max_features=None, max_leaf_nodes=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, random_state=2477,
                                        splitter='best'))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000257F9505CF0>)
2024-03-15 11:10:27,554:INFO:Checking exceptions
2024-03-15 11:10:27,554:INFO:Preloading libraries
2024-03-15 11:10:27,555:INFO:Set up data.
2024-03-15 11:10:27,565:INFO:Set up index.
2024-03-15 11:10:51,882:INFO:Initializing create_model()
2024-03-15 11:10:51,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 11:10:51,882:INFO:Checking exceptions
2024-03-15 11:10:51,902:INFO:Importing libraries
2024-03-15 11:10:51,903:INFO:Copying training dataset
2024-03-15 11:10:51,909:INFO:Defining folds
2024-03-15 11:10:51,910:INFO:Declaring metric variables
2024-03-15 11:10:51,914:INFO:Importing untrained model
2024-03-15 11:10:51,917:INFO:Random Forest Classifier Imported successfully
2024-03-15 11:10:51,926:INFO:Starting cross validation
2024-03-15 11:10:51,927:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 11:10:52,374:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,380:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,382:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,388:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,409:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,419:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,419:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:52,419:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:54,637:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:54,758:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:10:54,763:INFO:Calculating mean and std
2024-03-15 11:10:54,763:INFO:Creating metrics dataframe
2024-03-15 11:10:54,770:INFO:Finalizing model
2024-03-15 11:10:54,917:INFO:Uploading results into container
2024-03-15 11:10:54,918:INFO:Uploading model into container now
2024-03-15 11:10:54,925:INFO:_master_model_container: 17
2024-03-15 11:10:54,925:INFO:_display_container: 6
2024-03-15 11:10:54,925:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2477, verbose=0,
                       warm_start=False)
2024-03-15 11:10:54,925:INFO:create_model() successfully completed......................................
2024-03-15 11:10:59,921:INFO:Initializing predict_model()
2024-03-15 11:10:59,923:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2477, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025780963E20>)
2024-03-15 11:10:59,923:INFO:Checking exceptions
2024-03-15 11:10:59,923:INFO:Preloading libraries
2024-03-15 11:10:59,924:INFO:Set up data.
2024-03-15 11:10:59,934:INFO:Set up index.
2024-03-15 11:11:51,612:INFO:Initializing create_model()
2024-03-15 11:11:51,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 11:11:51,612:INFO:Checking exceptions
2024-03-15 11:11:51,634:INFO:Importing libraries
2024-03-15 11:11:51,635:INFO:Copying training dataset
2024-03-15 11:11:51,641:INFO:Defining folds
2024-03-15 11:11:51,642:INFO:Declaring metric variables
2024-03-15 11:11:51,648:INFO:Importing untrained model
2024-03-15 11:11:51,653:INFO:Light Gradient Boosting Machine Imported successfully
2024-03-15 11:11:51,661:INFO:Starting cross validation
2024-03-15 11:11:51,663:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 11:11:52,972:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:52,979:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:52,987:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:52,999:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,025:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,025:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,025:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,040:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,040:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,040:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:11:53,064:INFO:Calculating mean and std
2024-03-15 11:11:53,064:INFO:Creating metrics dataframe
2024-03-15 11:11:53,078:INFO:Finalizing model
2024-03-15 11:11:53,798:INFO:Uploading results into container
2024-03-15 11:11:53,799:INFO:Uploading model into container now
2024-03-15 11:11:53,823:INFO:_master_model_container: 18
2024-03-15 11:11:53,823:INFO:_display_container: 8
2024-03-15 11:11:53,824:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2477, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2024-03-15 11:11:53,824:INFO:create_model() successfully completed......................................
2024-03-15 11:12:24,425:INFO:Initializing predict_model()
2024-03-15 11:12:24,425:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2477, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002578395A710>)
2024-03-15 11:12:24,425:INFO:Checking exceptions
2024-03-15 11:12:24,425:INFO:Preloading libraries
2024-03-15 11:12:24,428:INFO:Set up data.
2024-03-15 11:12:24,438:INFO:Set up index.
2024-03-15 11:42:26,964:INFO:Initializing create_model()
2024-03-15 11:42:26,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-03-15 11:42:26,964:INFO:Checking exceptions
2024-03-15 11:42:27,009:INFO:Importing libraries
2024-03-15 11:42:27,010:INFO:Copying training dataset
2024-03-15 11:42:27,019:INFO:Defining folds
2024-03-15 11:42:27,019:INFO:Declaring metric variables
2024-03-15 11:42:27,030:INFO:Importing untrained model
2024-03-15 11:42:27,039:INFO:Extreme Gradient Boosting Imported successfully
2024-03-15 11:42:27,054:INFO:Starting cross validation
2024-03-15 11:42:27,054:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-03-15 11:42:33,175:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,203:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,210:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,252:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,255:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,258:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,260:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,263:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,287:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,314:WARNING:C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "C:\Users\salvi\anaconda3\lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\frame.py", line 3813, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6070, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\salvi\anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 6133, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Unnamed: 0'] not in index"

  warnings.warn(

2024-03-15 11:42:33,328:INFO:Calculating mean and std
2024-03-15 11:42:33,335:INFO:Creating metrics dataframe
2024-03-15 11:42:33,344:INFO:Finalizing model
2024-03-15 11:42:33,745:INFO:Uploading results into container
2024-03-15 11:42:33,745:INFO:Uploading model into container now
2024-03-15 11:42:33,758:INFO:_master_model_container: 19
2024-03-15 11:42:33,759:INFO:_display_container: 10
2024-03-15 11:42:33,760:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='multi:softprob', predictor=None, ...)
2024-03-15 11:42:33,760:INFO:create_model() successfully completed......................................
2024-03-15 11:42:36,996:INFO:Initializing predict_model()
2024-03-15 11:42:36,996:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000257F02951B0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='multi:softprob', predictor=None, ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000257FA0036D0>)
2024-03-15 11:42:36,996:INFO:Checking exceptions
2024-03-15 11:42:36,996:INFO:Preloading libraries
2024-03-15 11:42:37,000:INFO:Set up data.
2024-03-15 11:42:37,014:INFO:Set up index.
